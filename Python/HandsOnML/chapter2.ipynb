{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597760573512",
   "display_name": "Python 3.8.5 64-bit ('da38': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "pandas version: 1.1.0\nnumpy version: 1.19.1\nmatplotlib version: 3.3.0\nseaborn version: 0.10.1\nsklearn version: 0.23.2\nscipy version: 1.5.0\n"
    }
   ],
   "source": [
    "############################\n",
    "## import packages\n",
    "############################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "print('pandas version:', pd.__version__)\n",
    "print('numpy version:', np.__version__)\n",
    "print('matplotlib version:', matplotlib.__version__)\n",
    "print('seaborn version:', sns.__version__)\n",
    "print('sklearn version:', sklearn.__version__)\n",
    "print('scipy version:', scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "## Get data\n",
    "############################\n",
    "import os\n",
    "\n",
    "# data location and path\n",
    "DATA_URL = 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz'\n",
    "DATA_FOLDER = 'C:\\\\Users\\\\Admin\\\\Documents\\\\Data Analysis\\\\Data\\\\Housing data'\n",
    "DATA_FILE = 'housing.tgz'\n",
    "\n",
    "# function to download data\n",
    "def get_data(url=DATA_URL, path=DATA_FOLDER, file=DATA_FILE):\n",
    "    \"\"\"Downloads & Extracts the contents of `DATA_URL` into `DATA_FOLDER`\n",
    "    # Arguments:\n",
    "        url, string: the download link\n",
    "        path, string: where to download & extract data\n",
    "        file, string: downloaded filename\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    import urllib\n",
    "    if not os.path.exists(path):                        # create data folder if absent\n",
    "        os.makedirs(path)\n",
    "    tgz_path = os.path.join(path, file)\n",
    "    if not os.path.exists(tgz_path):                    # download data file if absent\n",
    "        urllib.request.urlretrieve(url=url, filename=tgz_path)\n",
    "    file_tgz = tarfile.open(name=tgz_path)              # extract tar-zipped file\n",
    "    file_tgz.extractall(path)\n",
    "    file_tgz.close()\n",
    "\n",
    "# function to load data\n",
    "FILE = 'housing.csv'\n",
    "def load_data(path=DATA_FOLDER, file=FILE):\n",
    "    \"\"\"Loads data into a pandas dataframe.\n",
    "    # Arguments:\n",
    "        path, string: the path where data lives\n",
    "        file, string: extacted filename\n",
    "    # Returns:\n",
    "        data, pd.DataFrame: data as a pandas dataframe\n",
    "    \"\"\"\n",
    "    get_data()\n",
    "    csv_path = os.path.join(path, file)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88                41.0        880.0           129.0   \n1    -122.22     37.86                21.0       7099.0          1106.0   \n2    -122.24     37.85                52.0       1467.0           190.0   \n3    -122.25     37.85                52.0       1274.0           235.0   \n4    -122.25     37.85                52.0       1627.0           280.0   \n\n   population  households  median_income  median_house_value ocean_proximity  \n0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n4       565.0       259.0         3.8462            342200.0        NEAR BAY  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "##########################\n",
    "# load and inspect data\n",
    "##########################\n",
    "housing = load_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n"
    }
   ],
   "source": [
    "housing.info()                  # reveals null counts if any ('total_bedrooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "longitude               844\nlatitude                862\nhousing_median_age       52\ntotal_rooms            5926\ntotal_bedrooms         1923\npopulation             3888\nhouseholds             1815\nmedian_income         12928\nmedian_house_value     3842\nocean_proximity           5\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "housing.nunique()   # reveals hidden categorical features (numeric type but with low unique counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64"
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "housing['ocean_proximity'].value_counts()       # category frequencies in 'ocean_proximity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          longitude      latitude  housing_median_age   total_rooms  \\\ncount  20640.000000  20640.000000        20640.000000  20640.000000   \nmean    -119.569704     35.631861           28.639486   2635.763081   \nstd        2.003532      2.135952           12.585558   2181.615252   \nmin     -124.350000     32.540000            1.000000      2.000000   \n25%     -121.800000     33.930000           18.000000   1447.750000   \n50%     -118.490000     34.260000           29.000000   2127.000000   \n75%     -118.010000     37.710000           37.000000   3148.000000   \nmax     -114.310000     41.950000           52.000000  39320.000000   \n\n       total_bedrooms    population    households  median_income  \\\ncount    20433.000000  20640.000000  20640.000000   20640.000000   \nmean       537.870553   1425.476744    499.539680       3.870671   \nstd        421.385070   1132.462122    382.329753       1.899822   \nmin          1.000000      3.000000      1.000000       0.499900   \n25%        296.000000    787.000000    280.000000       2.563400   \n50%        435.000000   1166.000000    409.000000       3.534800   \n75%        647.000000   1725.000000    605.000000       4.743250   \nmax       6445.000000  35682.000000   6082.000000      15.000100   \n\n       median_house_value  \ncount        20640.000000  \nmean        206855.816909  \nstd         115395.615874  \nmin          14999.000000  \n25%         119600.000000  \n50%         179700.000000  \n75%         264725.000000  \nmax         500001.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20433.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-119.569704</td>\n      <td>35.631861</td>\n      <td>28.639486</td>\n      <td>2635.763081</td>\n      <td>537.870553</td>\n      <td>1425.476744</td>\n      <td>499.539680</td>\n      <td>3.870671</td>\n      <td>206855.816909</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.003532</td>\n      <td>2.135952</td>\n      <td>12.585558</td>\n      <td>2181.615252</td>\n      <td>421.385070</td>\n      <td>1132.462122</td>\n      <td>382.329753</td>\n      <td>1.899822</td>\n      <td>115395.615874</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-124.350000</td>\n      <td>32.540000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.499900</td>\n      <td>14999.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-121.800000</td>\n      <td>33.930000</td>\n      <td>18.000000</td>\n      <td>1447.750000</td>\n      <td>296.000000</td>\n      <td>787.000000</td>\n      <td>280.000000</td>\n      <td>2.563400</td>\n      <td>119600.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-118.490000</td>\n      <td>34.260000</td>\n      <td>29.000000</td>\n      <td>2127.000000</td>\n      <td>435.000000</td>\n      <td>1166.000000</td>\n      <td>409.000000</td>\n      <td>3.534800</td>\n      <td>179700.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-118.010000</td>\n      <td>37.710000</td>\n      <td>37.000000</td>\n      <td>3148.000000</td>\n      <td>647.000000</td>\n      <td>1725.000000</td>\n      <td>605.000000</td>\n      <td>4.743250</td>\n      <td>264725.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-114.310000</td>\n      <td>41.950000</td>\n      <td>52.000000</td>\n      <td>39320.000000</td>\n      <td>6445.000000</td>\n      <td>35682.000000</td>\n      <td>6082.000000</td>\n      <td>15.000100</td>\n      <td>500001.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\n#####################################\\n### quick look and feel\\n#####################################\\n# histogram of all numerical features\\nhousing.hist(bins=50, figsize=(15,10))\\nplt.show()\\n# -> spike at the right edge of house age and price distributions => data capped ->\\n# remove those districts from training set (to prevent model from being evaluated poorly if it\\n# predicts values beyond cap)\\n# -> many distributions are right-tail-heavy => hard for model to detect patterns ->\\n# data may need transforming to generate more bell-shaped distributions\\n# -> features have different scales => data normalization is needed\\n'"
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "source": [
    "\"\"\"\n",
    "#####################################\n",
    "### quick look and feel\n",
    "#####################################\n",
    "# histogram of all numerical features\n",
    "housing.hist(bins=50, figsize=(15,10))\n",
    "plt.show()\n",
    "# -> spike at the right edge of house age and price distributions => data capped ->\n",
    "# remove those districts from training set (to prevent model from being evaluated poorly if it\n",
    "# predicts values beyond cap)\n",
    "# -> many distributions are right-tail-heavy => hard for model to detect patterns ->\n",
    "# data may need transforming to generate more bell-shaped distributions\n",
    "# -> features have different scales => data normalization is needed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\n# pairwise scatterplots of numerical features\\nfrom scipy.stats import pearsonr\\ndef reg_coef(x, y, label=None, color=None, **kwargs):\\n    ax = plt.gca()\\n    r,p = pearsonr(x,y)\\n    ax.annotate('r = {:.2f}'.format(r), xy=(0.5,0.5), fontsize=14, xycoords='axes fraction', ha='center')\\n    ax.set_axis_off()\\n\\nfeatureSet = ['housing_median_age','total_rooms','population','households','median_income','median_house_value']\\nsns.set_style('ticks')\\ng = sns.PairGrid(housing[featureSet], diag_sharey=False, height=1.5, aspect=1.4)\\ng.map_diag(sns.kdeplot, shade=True)\\ng.map_lower(sns.scatterplot, s=30, edgecolor='w')\\n#g.map_lower(sns.regplot, scatter_kws={'s':15})\\ng.map_upper(reg_coef)\\nplt.show()\\n\""
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "\"\"\"\n",
    "# pairwise scatterplots of numerical features\n",
    "from scipy.stats import pearsonr\n",
    "def reg_coef(x, y, label=None, color=None, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    r,p = pearsonr(x,y)\n",
    "    ax.annotate('r = {:.2f}'.format(r), xy=(0.5,0.5), fontsize=14, xycoords='axes fraction', ha='center')\n",
    "    ax.set_axis_off()\n",
    "\n",
    "featureSet = ['housing_median_age','total_rooms','population','households','median_income','median_house_value']\n",
    "sns.set_style('ticks')\n",
    "g = sns.PairGrid(housing[featureSet], diag_sharey=False, height=1.5, aspect=1.4)\n",
    "g.map_diag(sns.kdeplot, shade=True)\n",
    "g.map_lower(sns.scatterplot, s=30, edgecolor='w')\n",
    "#g.map_lower(sns.regplot, scatter_kws={'s':15})\n",
    "g.map_upper(reg_coef)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\n# zooming in income-house price scatterplot\\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.5, figsize=(10,7))\\nplt.show()\\n# plot shows cap at 500000, also few horizontal lines at ~450000, 350000, 280000 etc ->\\n# these data points should be removed to prevent model from learning to predict similar quirks\\n'"
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "\"\"\"\n",
    "# zooming in income-house price scatterplot\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.5, figsize=(10,7))\n",
    "plt.show()\n",
    "# plot shows cap at 500000, also few horizontal lines at ~450000, 350000, 280000 etc ->\n",
    "# these data points should be removed to prevent model from learning to predict similar quirks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3    0.350581\n2    0.318847\n4    0.176308\n5    0.114438\n1    0.039826\nName: income_cat, dtype: float64"
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "##############################################\n",
    "### stratified sampling\n",
    "##############################################\n",
    "# because price is related to income, strata (hidden clusters) in income distribution will be used\n",
    "# to generate stratified training and test sets ->\n",
    "# use pd.cut() to create a discrete set of income strata [1, 2, 3, 4, 5] and merge all\n",
    "# incomes above 5 into stratum 5\n",
    "housing['income_cat'] = pd.cut(x=housing['median_income'], bins=[0, 1.5, 3, 4.5, 6, np.inf],                                      labels=[1, 2, 3, 4, 5])\n",
    "housing['income_cat'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(X=housing, y=housing['income_cat']):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "# both train and test sets have same income strata proportions as the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3    0.350594\n2    0.318859\n4    0.176296\n5    0.114402\n1    0.039850\nName: income_cat, dtype: float64"
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "strat_train_set['income_cat'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3    0.350533\n2    0.318798\n4    0.176357\n5    0.114583\n1    0.039729\nName: income_cat, dtype: float64"
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "strat_test_set['income_cat'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((16512, 10), (4128, 10))"
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "# next remove unneeded 'income_cat' column both sets\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop('income_cat', axis=1, inplace=True)\n",
    "\n",
    "strat_train_set.shape, strat_test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "### visualize training data set\n",
    "#####################################\n",
    "# keep away test data and explore only training dataset (use a copy to prevent data corrpuption)\n",
    "# (install pyarrow with <conda install pyarrow> if needed)\n",
    "strat_test_set.reset_index().to_feather(fname='strat_test_set.f')\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\n# scatterplot of geographical distribution of housing based on latitude and logitude\\nhousing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1, figsize=(10,7))\\nplt.show()\\n# plot shows California coastline, and using alpha=0.1 highlights high-density areas\\n\""
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "\"\"\"\n",
    "# scatterplot of geographical distribution of housing based on latitude and logitude\n",
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1, figsize=(10,7))\n",
    "plt.show()\n",
    "# plot shows California coastline, and using alpha=0.1 highlights high-density areas\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\nhousing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\\n             s=housing['population']/100, label='population', c='median_house_value',\\n             cmap=plt.get_cmap('jet'), colorbar=True, figsize=(10,7))\\nplt.legend()\\nplt.show()\\n# plot highlights relation between housing price, location (ocean proximity) and density\\n\""
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "\"\"\"\n",
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\n",
    "             s=housing['population']/100, label='population', c='median_house_value',\n",
    "             cmap=plt.get_cmap('jet'), colorbar=True, figsize=(10,7))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plot highlights relation between housing price, location (ocean proximity) and density\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "median_house_value          1.000000\nmedian_income               0.687160\nrooms_per_household         0.146285\ntotal_rooms                 0.135097\nhousing_median_age          0.114110\nhouseholds                  0.064506\ntotal_bedrooms              0.047689\npopulation_per_household   -0.021985\npopulation                 -0.026920\nlongitude                  -0.047432\nlatitude                   -0.142724\nbedrooms_per_room          -0.259984\nName: median_house_value, dtype: float64"
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "#####################################\n",
    "## explore feature combination\n",
    "#####################################\n",
    "# -> total rooms in a district is uninformative - room/household is better\n",
    "# -> total bedrooms uninformative - comparing with toal rooms is better\n",
    "# -> total population is less information - pop/household is better\n",
    "housing['rooms_per_household'] = housing['total_rooms']/housing['households']\n",
    "housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms']\n",
    "housing['population_per_household'] = housing['population']/housing['households']\n",
    "\n",
    "# verify with correlation matrix\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)\n",
    "# 'bedrooms_per_room' has stronegr correlation with price than 'total_bedrooms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((16512, 9), (16512,))"
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "########################################\n",
    "## prep data for ML algorithms\n",
    "########################################\n",
    "# separate training dataset between features/predictors and label/target\n",
    "housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "housing_labels = strat_train_set['median_house_value'].copy()\n",
    "housing.shape, housing_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -121.89     37.29                38.0       1568.0           351.0   \n1    -121.93     37.05                14.0        679.0           108.0   \n2    -117.20     32.77                31.0       1952.0           471.0   \n3    -119.61     36.31                25.0       1847.0           371.0   \n4    -118.59     34.23                17.0       6592.0          1525.0   \n\n   population  households  median_income  \n0       710.0       339.0         2.7042  \n1       306.0       113.0         6.4214  \n2       936.0       462.0         2.8621  \n3      1460.0       353.0         1.8839  \n4      4459.0      1463.0         3.0347  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-121.89</td>\n      <td>37.29</td>\n      <td>38.0</td>\n      <td>1568.0</td>\n      <td>351.0</td>\n      <td>710.0</td>\n      <td>339.0</td>\n      <td>2.7042</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-121.93</td>\n      <td>37.05</td>\n      <td>14.0</td>\n      <td>679.0</td>\n      <td>108.0</td>\n      <td>306.0</td>\n      <td>113.0</td>\n      <td>6.4214</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-117.20</td>\n      <td>32.77</td>\n      <td>31.0</td>\n      <td>1952.0</td>\n      <td>471.0</td>\n      <td>936.0</td>\n      <td>462.0</td>\n      <td>2.8621</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-119.61</td>\n      <td>36.31</td>\n      <td>25.0</td>\n      <td>1847.0</td>\n      <td>371.0</td>\n      <td>1460.0</td>\n      <td>353.0</td>\n      <td>1.8839</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-118.59</td>\n      <td>34.23</td>\n      <td>17.0</td>\n      <td>6592.0</td>\n      <td>1525.0</td>\n      <td>4459.0</td>\n      <td>1463.0</td>\n      <td>3.0347</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "# impute missing 'total_bedrooms' data with median\n",
    "from sklearn.impute import SimpleImputer\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)   # drop categorical feature from median computation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(housing_num)            # computes median for ALL numeric features\n",
    "#housing_num.median().values        # shows median values of all numeric features\n",
    "X = imputer.transform(housing_num)  # impute ALL missing data with median (output is numpy array)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)   # convert back to dataframe\n",
    "housing_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       ...,\n       [0., 1., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0.]])"
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "# one-hot encoding to transform categorical 'ocean_proximity' to numerical (binary) variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "housing_cat = housing[['ocean_proximity']]\n",
    "encoder = OneHotEncoder()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat.values)\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-121.89, 37.29, 38.0, 1568.0, 351.0, 710.0, 339.0, 2.7042,\n        '<1H OCEAN'],\n       [-121.93, 37.05, 14.0, 679.0, 108.0, 306.0, 113.0, 6.4214,\n        '<1H OCEAN'],\n       [-117.2, 32.77, 31.0, 1952.0, 471.0, 936.0, 462.0, 2.8621,\n        'NEAR OCEAN'],\n       [-119.61, 36.31, 25.0, 1847.0, 371.0, 1460.0, 353.0, 1.8839,\n        'INLAND'],\n       [-118.59, 34.23, 17.0, 6592.0, 1525.0, 4459.0, 1463.0, 3.0347,\n        '<1H OCEAN']], dtype=object)"
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "# Custom Transformer class to add combined attributes to data sets\n",
    "# (to work seamlessly with sklearn functionalities, such as pipelines, this class\n",
    "# must implement three methods: 'fit()', 'transform()' and 'fit_transform()')\n",
    "# -> adding TransformerMixin as a base class gives method 'fit_transform()' for free\n",
    "# -> adding BaseEstimator as a base class and avoiding args and *kwargs in Constructor\n",
    "#    gives two extra methods '.get_params()' and '.set_params()'\n",
    "#    (useful for auto-tuning hyperparameters)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Engineers new features from existing ones: `rooms_per_household`,            \n",
    "    `population_per_household`, `bedrooms_per_room`\n",
    "    # Arguments:\n",
    "        add_bedrooms_per_room, bool: defaults to True. Indicates if we want to add the feature  \n",
    "        `bedrooms_per_room`.\n",
    "    \"\"\"\n",
    "    def __init__(self, add_bedrooms_per_room=True):         # no args or *kwargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"returns self\"\"\"          # Docstring for fit() method\n",
    "        return self         # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"returns Numpy array of data with added attributes\"\"\"\n",
    "        rooms_per_household = X[:,rooms_ix] / X[:,household_ix]\n",
    "        population_per_household = X[:,population_ix] / X[:,household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:,bedrooms_ix] / X[:,rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "housing.values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-121.89, 37.29, 38.0, 1568.0, 351.0, 710.0, 339.0, 2.7042,\n        '<1H OCEAN', 4.625368731563422, 2.094395280235988],\n       [-121.93, 37.05, 14.0, 679.0, 108.0, 306.0, 113.0, 6.4214,\n        '<1H OCEAN', 6.008849557522124, 2.7079646017699117],\n       [-117.2, 32.77, 31.0, 1952.0, 471.0, 936.0, 462.0, 2.8621,\n        'NEAR OCEAN', 4.225108225108225, 2.0259740259740258],\n       [-119.61, 36.31, 25.0, 1847.0, 371.0, 1460.0, 353.0, 1.8839,\n        'INLAND', 5.232294617563739, 4.135977337110481],\n       [-118.59, 34.23, 17.0, 6592.0, 1525.0, 4459.0, 1463.0, 3.0347,\n        '<1H OCEAN', 4.50580997949419, 3.047846889952153]], dtype=object)"
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "housing_extra_attribs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(16512, 16)"
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "##############################################################################################\n",
    "# Data transformation using pipeline\n",
    "# Pipeline constructor takes a list of (name,estimator) pairs defining a sequence of steps;\n",
    "# all but last estimator must be transformers (must have a 'fit_transform()' method).\n",
    "# Calling pipeline's 'fit()' method calls 'fit_transform()' sequentially on all transformers,\n",
    "# passing output of each call as parameter to next call, until it reaches final estimator,\n",
    "# for which it just calls 'fit()' method.\n",
    "# Pipeline exposes same methods as the last estimator: in this example, StandardScaler is a\n",
    "# transformer, so pipeline has a 'transform()' method that applies to all transforms to data\n",
    "# in sequence (it also has a 'fit_transform()' that we could call instead of calling 'fit()'\n",
    "# and then 'transform()')\n",
    "##############################################################################################\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# pipeline to transform numeric data\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scalar', StandardScaler())\n",
    "])\n",
    "\n",
    "# full pipeline (ColumnTransformer transforms both numeric and categorical columns)\n",
    "num_attribs = housing_num.columns.tolist()      # numeric feature names\n",
    "cat_attribs = ['ocean_proximity']               # categorical feature name\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', OneHotEncoder(), cat_attribs)\n",
    "])\n",
    "\n",
    "housing_prepped = full_pipeline.fit_transform(X = housing)\n",
    "housing_prepped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction:\t [210644.60459286 317768.80697211 210956.43331178  59218.98886849\n 189747.55849879]\nLabels:\t [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n"
    }
   ],
   "source": [
    "###################################################################\n",
    "# Select and train model\n",
    "###################################################################\n",
    "# first a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X=housing_prepped, y=housing_labels)\n",
    "\n",
    "# Example below does NOT work when full_pipeline.fit_transform() is used with 'some_data'.\n",
    "# some_data has only 5 rows of data -> 'ocean_proximity' has 3 values (instead of 5 in\n",
    "# the full housing data). This gives different dimensionality to some_data_prepped when created\n",
    "# with full_pipeline.fit_transform(), which throws error with lin_reg.predict(), because\n",
    "# lin_reg was fitted with training data 'housing_prepped'. Use full_pipeline.transform() instead\n",
    "# (this works because full_pipeline transforms some_data without fitting to it first - it uses\n",
    "# earlier fit with housing data instead).\n",
    "# fit(), fit_transform(), fit_predict() should only be called with training data, never on\n",
    "# validation/test/new data (https://github.com/ageron/handson-ml/issues/347#issuecomment-501558848).\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepped = full_pipeline.transform(some_data)\n",
    "print('Prediction:\\t', lin_reg.predict(some_data_prepped))\n",
    "print('Labels:\\t', list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "68628.19819848923"
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "# evaluate model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepped)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0"
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "# next a Decision Tree Regressor model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X=housing_prepped, y=housing_labels)\n",
    "\n",
    "housing_predictions = tree_reg.predict(housing_prepped)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse\n",
    "# model overfits training data - cross validation (below) is the way to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "18692.87600982754"
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "# next Random Forest Regressor model (Ensemble Learning - trains many Decision Trees)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(X=housing_prepped, y=housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepped)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "111094.6308539982"
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "# next a Support Vector Machine Regressor model\n",
    "# (with linear kernel)\n",
    "from sklearn.svm import SVR\n",
    "svr_reg = SVR(kernel='linear')\n",
    "svr_reg.fit(X=housing_prepped, y=housing_labels)\n",
    "\n",
    "housing_predictions = svr_reg.predict(housing_prepped)\n",
    "svr_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "svr_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "118580.68301157995"
     },
     "metadata": {},
     "execution_count": 168
    }
   ],
   "source": [
    "# (with rbf kernel)\n",
    "svr_reg = SVR(kernel='rbf')\n",
    "svr_reg.fit(X=housing_prepped, y=housing_labels)\n",
    "\n",
    "housing_predictions = svr_reg.predict(housing_prepped)\n",
    "svr_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "svr_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Scores: [68301.73432572 66411.83830823 73067.99662965 68044.75268659\n 71386.18356629 72832.52831767 72070.00510292 71568.07122127\n 76271.78301862 69601.9569938 ]\nMean: 70955.68501707369\nStandard deviation: 2757.5978311315357\n"
    }
   ],
   "source": [
    "############################################\n",
    "# cross validation (k-fold)\n",
    "############################################\n",
    "# Decision Tree model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(estimator = tree_reg,\n",
    "                         X = housing_prepped,\n",
    "                         y = housing_labels,\n",
    "                         scoring = 'neg_mean_squared_error',\n",
    "                         cv = 10        # 10-fold CV\n",
    "                        )\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "# cross-validation expects utility function (greater the better) rather than a cost function\n",
    "# (lower the better), hence scoring function is opposite of MSE (i.e. negative) -> output scores\n",
    "# is negated to compute RMSE \n",
    "\n",
    "# function to display scores for k-fold CV\n",
    "def display_scores(scores):\n",
    "    \"\"\"Displays the scores, their mean, and the standard deviation.\n",
    "    \n",
    "    # Arguments:\n",
    "        scores, np.array: list of scores given by the cross validation procedure.\n",
    "    \"\"\"\n",
    "    print('Scores:', scores)\n",
    "    print('Mean:', scores.mean())\n",
    "    print('Standard deviation:', scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Scores: [66782.73843989 66960.118071   70347.95244419 74739.57052552\n 68031.13388938 71193.84183426 64969.63056405 68281.61137997\n 71552.91566558 67665.10082067]\nMean: 69052.46136345083\nStandard deviation: 2731.6740017983493\n"
    }
   ],
   "source": [
    "# linear regression model\n",
    "scores = cross_val_score(estimator = lin_reg,\n",
    "                         X = housing_prepped,\n",
    "                         y = housing_labels,\n",
    "                         scoring = 'neg_mean_squared_error',\n",
    "                         cv = 10\n",
    "                        )\n",
    "lin_rmse_scores = np.sqrt(-scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Scores: [49074.49312152 47784.3286393  49692.11740759 52207.66648538\n 49500.82478611 53271.7779091  49246.28821604 47846.79675513\n 52973.56159263 49868.17461888]\nMean: 50146.60295316947\nStandard deviation: 1885.1243998022412\n"
    }
   ],
   "source": [
    "# Random forest model\n",
    "scores = cross_val_score(estimator = forest_reg,\n",
    "                         X = housing_prepped,\n",
    "                         y = housing_labels,\n",
    "                         scoring = 'neg_mean_squared_error',\n",
    "                         cv = 10\n",
    "                        )\n",
    "forest_rmse_scores = np.sqrt(-scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['models/forest_reg.m']"
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "source": [
    "####################################################################\n",
    "# save all model paramaters/hyperparameters, outputs, scores to file\n",
    "####################################################################\n",
    "import joblib\n",
    "path = 'models'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "joblib.dump(value=forest_reg, filename=path+'/forest_reg.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load model from file\n",
    "forest_reg = joblib.load(filename=path+'/forest_reg.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   15.9s\n[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   36.0s finished\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GridSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n             param_grid=[{'max_features': [2, 4, 6, 8],\n                          'n_estimators': [3, 10, 30]},\n                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n                          'n_estimators': [3, 10]}],\n             return_train_score=True, scoring='neg_mean_squared_error',\n             verbose=2)"
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "##########################################################\n",
    "# Fine tuning model with grid search (good for small grid)\n",
    "# use RandomizedSearchCV for large parameter space\n",
    "##########################################################\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# For Random Forest model ->\n",
    "# first set of evaluations in dict#1 uses 'bootstrap':[True] (default)\n",
    "# 3X4 + 2X3 = 18 combinations of hyperparameter values\n",
    "# 18 X 5 (for 5-fold CV) = 90 rounds of training\n",
    "param_grid = [\n",
    "    {'n_estimators':[3, 10, 30], 'max_features':[2, 4, 6, 8]},\n",
    "    {'bootstrap':[False], 'n_estimators':[3, 10], 'max_features':[2, 3, 4]}\n",
    "]\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_grid_search = GridSearchCV(estimator = forest_reg,\n",
    "                                  param_grid = param_grid,\n",
    "                                  scoring = 'neg_mean_squared_error',\n",
    "                                  cv = 5,\n",
    "                                  return_train_score = True,\n",
    "                                  n_jobs = -1,      # parallelizing jobs\n",
    "                                  verbose = 2\n",
    "                                 )\n",
    "forest_grid_search.fit(X=housing_prepped, y=housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "50081.87192950168"
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "best_mse = forest_grid_search.best_score_\n",
    "best_rmse = np.sqrt(-best_mse)\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestRegressor(max_features=8, n_estimators=30)"
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "forest_grid_search.best_estimator_         # best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "63382.430727482824 {'max_features': 2, 'n_estimators': 3}\n55271.40238097063 {'max_features': 2, 'n_estimators': 10}\n53099.54207939212 {'max_features': 2, 'n_estimators': 30}\n60938.99698679386 {'max_features': 4, 'n_estimators': 3}\n52585.36726854069 {'max_features': 4, 'n_estimators': 10}\n50684.63919850133 {'max_features': 4, 'n_estimators': 30}\n59478.886807279996 {'max_features': 6, 'n_estimators': 3}\n52851.064798669955 {'max_features': 6, 'n_estimators': 10}\n50402.80562813449 {'max_features': 6, 'n_estimators': 30}\n58978.24373669876 {'max_features': 8, 'n_estimators': 3}\n52313.635784851656 {'max_features': 8, 'n_estimators': 10}\n50081.87192950168 {'max_features': 8, 'n_estimators': 30}\n62302.38383726054 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n54140.19442744822 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n59044.55365118484 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n52502.83442828373 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n59030.91627905687 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n51865.512833523106 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
    }
   ],
   "source": [
    "# evaluation scores for all 18 parameter combinations\n",
    "cvres = forest_grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.7min\n[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 14.5min\n[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 23.5min finished\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GridSearchCV(cv=5, estimator=SVR(), n_jobs=-1,\n             param_grid=[{'C': [10.0, 30.0, 100.0, 300.0, 1000.0, 3000.0,\n                                10000.0, 30000.0],\n                          'kernel': ['linear']},\n                         {'C': [1.0, 3.0, 10.0, 30.0, 100.0, 300.0, 1000.0],\n                          'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0],\n                          'kernel': ['rbf']}],\n             scoring='neg_mean_squared_error', verbose=2)"
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "# grid search for Support Vector Machine Regressor model ->\n",
    "param_grid = [\n",
    "    {\n",
    "     'kernel': ['linear'],\n",
    "     'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]\n",
    "    },\n",
    "    {\n",
    "     'kernel': ['rbf'],\n",
    "     'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "     'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]\n",
    "    }\n",
    "]\n",
    "svm_reg = SVR()\n",
    "svm_grid_search = GridSearchCV(estimator = svm_reg,\n",
    "                               param_grid = param_grid,\n",
    "                               cv = 5,\n",
    "                               scoring = \"neg_mean_squared_error\",\n",
    "                               n_jobs = -1,\n",
    "                               verbose = 2\n",
    "                              )\n",
    "svm_grid_search.fit(X=housing_prepped, y=housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "70363.84006944533"
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "source": [
    "best_mse = svm_grid_search.best_score_\n",
    "best_rmse = np.sqrt(-best_mse)\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVR(C=30000.0, kernel='linear')"
     },
     "metadata": {},
     "execution_count": 179
    }
   ],
   "source": [
    "svm_grid_search.best_estimator_\n",
    "# C is maxed -> should fine-tune again with extended range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.3min\n[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  4.8min finished\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomizedSearchCV(cv=5, estimator=SVR(), n_jobs=-1,\n                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001CF0624D4F0>,\n                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001CF06187250>,\n                                        'kernel': ['linear', 'rbf']},\n                   scoring='neg_mean_squared_error', verbose=2)"
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "source": [
    "# Random search with SVR\n",
    "# (random search tends to find better hyper-parameters than grid search in the same amount of time)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, expon\n",
    "\n",
    "params_distribs = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': reciprocal(20, 200000),    # reciprocal distrib for unknown hyper-parameter range\n",
    "    'gamma': expon(scale=1)         # exponential distrib for roughly known range\n",
    "}\n",
    "svm_reg = SVR()\n",
    "svm_random_search = RandomizedSearchCV(estimator = svm_reg,\n",
    "                                       param_distributions = params_distribs,\n",
    "                                       scoring = 'neg_mean_squared_error',\n",
    "                                       n_jobs = -1,\n",
    "                                       cv = 5,\n",
    "                                       verbose = 2\n",
    "                                      )\n",
    "svm_random_search.fit(X=housing_prepped, y=housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "57221.11478834314"
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "best_mse = svm_random_search.best_score_\n",
    "best_rmse = np.sqrt(-best_mse)\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'C': 33848.3437841147, 'gamma': 0.3379421632730737, 'kernel': 'rbf'}"
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "svm_random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(0.3577453640131463, 'median_income'),\n (0.1536589802745481, 'INLAND'),\n (0.11215522928115201, 'pop_per_hhold'),\n (0.07128099912602268, 'longitude'),\n (0.06572611084928882, 'bedrooms_per_room'),\n (0.06424447898075396, 'latitude'),\n (0.05647895177709007, 'rooms_per_hhold'),\n (0.04264960849183562, 'housing_median_age'),\n (0.01664738453614082, 'total_rooms'),\n (0.015504779169027988, 'population'),\n (0.01501865087728952, 'total_bedrooms'),\n (0.014812134473503248, 'households'),\n (0.00891297084265308, '<1H OCEAN'),\n (0.003061443288374794, 'NEAR OCEAN'),\n (0.0019608936961452053, 'NEAR BAY'),\n (0.0001420203230277317, 'ISLAND')]"
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "################################################################\n",
    "# Feature importance\n",
    "################################################################\n",
    "# use best Random Forest model to get feature importance (not available with SVR)\n",
    "feature_importances = forest_grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\n",
    "cat_encoder = full_pipeline.named_transformers_['cat']\n",
    "cat_1hot_attribs = cat_encoder.categories_[0].tolist()\n",
    "attributes = num_attribs + extra_attribs + cat_1hot_attribs\n",
    "#dict(zip(feature_importances, attributes))\n",
    "sorted(zip(feature_importances, attributes), reverse=True)\n",
    "# drop unimportant features if any (e.g. only one 'ocean_proximity' category is really useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.071281  , 0.35774536, 0.11215523, 0.06572611, 0.15365898])"
     },
     "metadata": {},
     "execution_count": 275
    }
   ],
   "source": [
    "##############################################################\n",
    "## Re-run SVR model with top few most important features\n",
    "##############################################################\n",
    "# first add a transformer to pipeline to select top features\n",
    "\n",
    "# function to get indices to top features ->\n",
    "# 'np.argpartition(arr,k)' function is used to create a indirect partitioned copy of array 'arr' \n",
    "# with its elements rearranged in such a way that the value of the element in k-th position is in \n",
    "# the position it would be in sorted 'arr'. All elements smaller than the k-th element are moved \n",
    "# before and all equal or greater are moved after. The ordering of the elements in the two \n",
    "# partitions is undefined. It returns an array of indices of the same shape as arr.\n",
    "# 'np.argpartition(np.array(arr), -k)[-k:]' below returns indices of top k elements of arr\n",
    "def indices_of_top_k(arr, k):\n",
    "    \"\"\" returns indices of top k elements of input array\n",
    "    \"\"\"\n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "ix = indices_of_top_k(feature_importances, 5)\n",
    "feature_importances[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Selects most important features\n",
    "    # Arguments:\n",
    "        feature_importances, Numpy array: values showing importance of each feature;\n",
    "        k, integer: number of features to be selected.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_importances, k):\n",
    "        self.feature_importances = feature_importances\n",
    "        self.k = k\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_indices_ = indices_of_top_k(arr=self.feature_importances, k=self.k)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[:, self.feature_indices_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(16512, 5)"
     },
     "metadata": {},
     "execution_count": 276
    }
   ],
   "source": [
    "# create a pipeline that runs full_pipeline and adds feature selection of top k features\n",
    "k = 5\n",
    "prep_and_feature_selection_pipeline = Pipeline([\n",
    "    ('prep', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k))\n",
    "])\n",
    "\n",
    "housing_prepped_top_features = prep_and_feature_selection_pipeline.fit_transform(X = housing)\n",
    "housing_prepped_top_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('attribs_adder',\n                                                                   CombinedAttributesAdder()),\n                                                                  ('std_scalar',\n                                                                   StandardScaler())]),\n                                                  ['longitude', 'latitude',\n                                                   'housing_median_age',\n                                                   'total_rooms',\n                                                   'total_bedrooms',\n                                                   'population', 'households',\n                                                   'median_income']),\n                                                 ('cat', OneHotEncoder(),\n                                                  ['oce...\n                 TopFeatureSelector(feature_importances=array([7.12809991e-02, 6.42444790e-02, 4.26496085e-02, 1.66473845e-02,\n       1.50186509e-02, 1.55047792e-02, 1.48121345e-02, 3.57745364e-01,\n       5.64789518e-02, 1.12155229e-01, 6.57261108e-02, 8.91297084e-03,\n       1.53658980e-01, 1.42020323e-04, 1.96089370e-03, 3.06144329e-03]),\n                                    k=5)),\n                ('prediction',\n                 SVR(C=33848.3437841147, gamma=0.3379421632730737))])"
     },
     "metadata": {},
     "execution_count": 279
    }
   ],
   "source": [
    "# create a pipeline that runs full data prep and model prediction with SVR\n",
    "# (uses best SVR parameters from random_search above)\n",
    "prep_select_and_predict_pipeline = Pipeline([\n",
    "    ('prep', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
    "    ('prediction', SVR(**svm_random_search.best_params_))\n",
    "])\n",
    "\n",
    "prep_select_and_predict_pipeline.fit(X=housing, y=housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predictions:\t [184976.96963451 338145.80039496 171898.84848753  54997.7446296 ]\nTargets:\t [286600.0, 340600.0, 196900.0, 46300.0]\n"
    }
   ],
   "source": [
    "some_data = housing.iloc[:4]\n",
    "some_labels = housing_labels.iloc[:4]\n",
    "print(\"Predictions:\\t\", prep_select_and_predict_pipeline.predict(some_data))\n",
    "print(\"Targets:\\t\", some_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.7min\n[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 12.5min\n[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 21.3min finished\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('prep',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('attribs_adder',\n                                                                                          CombinedAttributesAdder()),\n                                                                                         ('std_scalar',\n                                                                                          StandardScaler())]),\n                                                                         ['longitude',\n                                                                          'latitude',\n                                                                          'housing_median_age',\n                                                                          'total_rooms',\n                                                                          'total_bedrooms',\n                                                                          'population',\n                                                                          'households',\n                                                                          'median_income']),...\n       5.64789518e-02, 1.12155229e-01, 6.57261108e-02, 8.91297084e-03,\n       1.53658980e-01, 1.42020323e-04, 1.96089370e-03, 3.06144329e-03]),\n                                                           k=5)),\n                                       ('prediction',\n                                        SVR(C=33848.3437841147,\n                                            gamma=0.3379421632730737))]),\n             n_jobs=-1,\n             param_grid=[{'feature_selection__k': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n                                                   10, 11, 12, 13, 14, 15, 16],\n                          'prep__num__imputer__strategy': ['mean', 'median',\n                                                           'most_frequent']}],\n             scoring='neg_mean_squared_error', verbose=2)"
     },
     "metadata": {},
     "execution_count": 286
    }
   ],
   "source": [
    "# auto-explore prep option for SVR with grid search\n",
    "# (uses prep_select_prediction pipeline from above)\n",
    "param_grid = [\n",
    "    {\n",
    "        'prep__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'feature_selection__k': list(range(1, len(feature_importances) + 1))\n",
    "    }\n",
    "]\n",
    "\n",
    "svm_grid_search_prep = GridSearchCV(estimator = prep_select_and_predict_pipeline,\n",
    "                                param_grid = param_grid, \n",
    "                                cv = 5,\n",
    "                                scoring = 'neg_mean_squared_error',\n",
    "                                verbose = 2,\n",
    "                                n_jobs = -1\n",
    "                               )\n",
    "svm_grid_search_prep.fit(X=housing, y=housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "56914.851962975175"
     },
     "metadata": {},
     "execution_count": 287
    }
   ],
   "source": [
    "best_mse = svm_grid_search_prep.best_score_\n",
    "best_rmse = np.sqrt(-best_mse)\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'feature_selection__k': 10, 'prep__num__imputer__strategy': 'most_frequent'}"
     },
     "metadata": {},
     "execution_count": 288
    }
   ],
   "source": [
    "svm_grid_search_prep.best_params_\n",
    "# auto-selects top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('attribs_adder',\n                                                                   CombinedAttributesAdder()),\n                                                                  ('std_scalar',\n                                                                   StandardScaler())]),\n                                                  ['longitude', 'latitude',\n                                                   'housing_median_age',\n                                                   'total_rooms',\n                                                   'total_bedrooms',\n                                                   'population', 'households',\n                                                   'median_income']),\n                                                 ('cat', OneHotEncoder(...\n                 TopFeatureSelector(feature_importances=array([7.12809991e-02, 6.42444790e-02, 4.26496085e-02, 1.66473845e-02,\n       1.50186509e-02, 1.55047792e-02, 1.48121345e-02, 3.57745364e-01,\n       5.64789518e-02, 1.12155229e-01, 6.57261108e-02, 8.91297084e-03,\n       1.53658980e-01, 1.42020323e-04, 1.96089370e-03, 3.06144329e-03]),\n                                    k=10)),\n                ('prediction',\n                 SVR(C=33848.3437841147, gamma=0.3379421632730737))])"
     },
     "metadata": {},
     "execution_count": 289
    }
   ],
   "source": [
    "svm_grid_search_prep.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "68139.43891327262"
     },
     "metadata": {},
     "execution_count": 291
    }
   ],
   "source": [
    "##############################################################\n",
    "## Evaluate best model on test set\n",
    "##############################################################\n",
    "#final_model = forest_grid_search.best_estimator_\n",
    "final_model = svm_grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop('median_house_value', axis=1)\n",
    "y_test = strat_test_set['median_house_value'].copy()\n",
    "\n",
    "# prep test data with transform() (NOT fit_transform())\n",
    "X_test_prepped = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepped)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([65736.61788422, 70460.36715754])"
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "source": [
    "# 95% confidence interval of score\n",
    "from scipy import stats\n",
    "confidence = .95\n",
    "squared_errors = (y_test - final_predictions) ** 2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), \n",
    "        scale=stats.sem(squared_errors)))"
   ]
  }
 ]
}